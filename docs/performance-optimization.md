# Performance Optimization Guide

–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

---

## –û–±–∑–æ—Ä

–°–∏—Å—Ç–µ–º–∞ –∏–º–µ–µ—Ç **3 —É—Ä–æ–≤–Ω—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**

1. **Sync Mode** (–±–∞–∑–æ–≤—ã–π) - `OllamaClient`
   - Sequential processing
   - Simple & reliable
   - ~5 req/s throughput

2. **Async Mode** (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π) - `AsyncOllamaClient`
   - Non-blocking I/O
   - Concurrent requests
   - **3-5x faster**
   - ~15-25 req/s

3. **Batch Mode** (–º–∞–∫—Å–∏–º—É–º) - `BatchProcessor`
   - Auto-batching
   - Priority queues
   - **5-10x faster**
   - ~50+ req/s

---

## Async/Await Optimization

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Async:**
- Multiple concurrent LLM calls
- High request rate (>10 req/s)
- I/O-bound operations
- Real-time responsiveness

‚ùå **–ù–µ –Ω—É–∂–µ–Ω Async:**
- Single sequential requests
- Low request rate (<5 req/s)
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏

### Setup

```bash
# Install async dependencies
pip install aiohttp
```

### Usage Example

```python
from core.async_ollama_client import AsyncOllamaClient
import asyncio

async def main():
    async with AsyncOllamaClient(cfg) as client:
        # Single request (same as sync)
        response = await client.chat("Hello")
        
        # Concurrent batch (3-5x faster!)
        prompts = [
            "Question 1",
            "Question 2",
            "Question 3",
        ]
        responses = await client.chat_batch(prompts)
        # All 3 processed concurrently!
        
        # Embedding batch (5-10x faster!)
        texts = ["text1", "text2", "text3"]
        embeddings = await client.embed_batch(texts)

asyncio.run(main())
```

### Performance Comparison

```python
# Sync version (sequential)
for prompt in prompts:  # 3 prompts @ 2s each = 6s
    result = ollama.chat(prompt)

# Async version (concurrent)
results = await async_ollama.chat_batch(prompts)  # ~2s total!
# 3x faster!
```

---

## Batch Processing

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Batching:**
- Bulk operations (10+ items)
- Variable arrival rate
- Need efficient queueing
- Priority processing

### Setup

```python
from core.batch_processor import AsyncBatchProcessor
from core.async_ollama_client import AsyncOllamaClient

# Create async client
async_client = AsyncOllamaClient(cfg)

# Create batch processor
processor = AsyncBatchProcessor(
    process_fn=async_client.chat_batch,
    batch_size=10,     # Process 10 at a time
    timeout=1.0,       # Max 1s wait
)

await processor.start()
```

### Usage

```python
# Submit items as they arrive
futures = []
for prompt in stream_of_prompts:
    future = processor.submit(prompt, priority=get_priority(prompt))
    futures.append(future)

# Wait for all results
results = await asyncio.gather(*futures)

# Auto-batched efficiently!
# High priority items processed first
```

### Batch Size Tuning

```yaml
# config.yaml
batch:
  # Small batches - lower latency
  batch_size: 5
  timeout: 0.5
  
  # Large batches - higher throughput
  batch_size: 20
  timeout: 2.0
  
  # Balanced (recommended)
  batch_size: 10
  timeout: 1.0
```

---

## Connection Pooling

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤ AsyncOllamaClient

```python
# Configured in AsyncOllamaClient
# TCP connection pool
connector = aiohttp.TCPConnector(
    limit=100,           # Max 100 connections
    ttl_dns_cache=300,   # DNS cache 5 min
)

# Reuses connections efficiently
# No connection overhead per request
```

### Tuning

```yaml
# config.yaml
async:
  max_connections: 100  # Default
  # Increase for high load:
  max_connections: 200
  # Decrease for low resources:
  max_connections: 50
```

---

## Cache Optimization

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

#### 1. Aggressive Caching (Max Performance)

```yaml
cache:
  max_size: 500        # Large cache
  ttl_seconds: 3600    # 1 hour TTL

# Use when:
# - High repeat rate
# - RAM available
# - Prompts stable

# Expected: 50-70% hit rate
```

#### 2. Balanced Caching (Recommended)

```yaml
cache:
  max_size: 200
  ttl_seconds: 600     # 10 min

# Use when:
# - Moderate repeat rate
# - Limited RAM
# - Mixed workload

# Expected: 30-50% hit rate
```

#### 3. Minimal Caching (Low Memory)

```yaml
cache:
  max_size: 50
  ttl_seconds: 300     # 5 min

# Use when:
# - Low repeat rate
# - Very limited RAM
# - Dynamic prompts

# Expected: 10-20% hit rate
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ cache

```python
stats = ollama.get_cache_stats()

print(f"Hit rate: {stats['hit_rate']}%")
print(f"Size: {stats['current_size']}/{stats['max_size']}")
print(f"Evictions: {stats['evictions']}")

# Tune based on hit rate:
# < 20%: Cache too small or TTL too short
# > 60%: Can increase size for more hits
```

---

## Rate Limiting Optimization

### –ü–æ–¥–±–æ—Ä limits

```yaml
# Conservative (stability > speed)
rate_limit:
  chat_rate: 5.0
  chat_burst: 10

# Balanced (recommended)
rate_limit:
  chat_rate: 10.0
  chat_burst: 20

# Aggressive (speed > stability)
rate_limit:
  chat_rate: 20.0
  chat_burst: 50

# Note: Depends on Ollama capacity!
# Test before deploying
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama capacity

```bash
# Load test Ollama
for i in {1..20}; do
  curl -X POST http://localhost:11434/api/chat \
    -d '{"model":"llama3.2","messages":[{"role":"user","content":"test"}]}' &
done
wait

# Monitor response times
# Tune rate_limit based on results
```

---

## Benchmarking

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ throughput

```python
import time
import asyncio
from core.async_ollama_client import AsyncOllamaClient

async def benchmark():
    async with AsyncOllamaClient(cfg) as client:
        prompts = ["Test prompt"] * 100
        
        start = time.time()
        results = await client.chat_batch(prompts)
        elapsed = time.time() - start
        
        print(f"Throughput: {len(prompts)/elapsed:.1f} req/s")
        print(f"Avg latency: {elapsed/len(prompts)*1000:.0f}ms")

asyncio.run(benchmark())
```

### –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import cProfile
import pstats

# Profile code
profiler = cProfile.Profile()
profiler.enable()

# Your code here
for _ in range(100):
    ollama.chat("test")

profiler.disable()

# Print stats
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 functions
```

---

## Performance Best Practices

### 1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π client

```python
# Low load (<5 req/s)
from core.ollama_client import OllamaClient
client = OllamaClient(cfg)

# Medium load (5-20 req/s)
from core.async_ollama_client import AsyncOllamaClient
client = AsyncOllamaClient(cfg)

# High load (20+ req/s)
from core.batch_processor import AsyncBatchProcessor
processor = AsyncBatchProcessor(...)
```

### 2. Batch –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ

```python
# –ü–ª–æ—Ö–æ
for item in items:
    result = await client.chat(item)
    # 10 items @ 2s = 20s

# –•–æ—Ä–æ—à–æ
results = await client.chat_batch(items)
# 10 items @ ~2s = 2s (10x faster!)
```

### 3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ cache

```yaml
# Monitor hit rate
# Adjust size/TTL accordingly
cache:
  max_size: 200  # Start here
  ttl_seconds: 600
```

### 4. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ metrics

```python
# Check regularly
stats = client.get_comprehensive_stats()

if stats['cache']['hit_rate'] < 20:
    log.warning("Low cache hit rate - increase size or TTL")

if stats['rate_limiters']['chat']['rejected'] > 0:
    log.warning("Rate limiting active - consider increasing limits")
```

### 5. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ prompts

```python
# –ü–ª–æ—Ö–æ - –¥–ª–∏–Ω–Ω—ã–µ prompts
prompt = """ (1000 words of context) """

# –•–æ—Ä–æ—à–æ - –∫–æ–Ω—Ü–∏–∑–Ω—ã–µ prompts
prompt = "Summarize: X"

# Shorter prompts = faster processing
```

---

## Performance Metrics

### –¶–µ–ª–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏

```yaml
Sync Mode:
  Throughput: 5-10 req/s
  P95 Latency: 2-5s
  Cache Hit Rate: 30-50%
  Memory: 500MB

Async Mode:
  Throughput: 15-25 req/s  # 3-5x
  P95 Latency: 1-3s
  Cache Hit Rate: 30-50%
  Memory: 600MB

Batch Mode:
  Throughput: 50+ req/s    # 10x+
  P95 Latency: 2-4s (per batch)
  Cache Hit Rate: 30-50%
  Memory: 700MB
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ queries

```promql
# Throughput
rate(llm_calls_total[1m])

# Latency P95
histogram_quantile(0.95, rate(llm_call_duration_seconds_bucket[5m]))

# Cache hit rate
100 * rate(cache_hits_total[5m]) / 
(rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))

# Queue size (batch processor)
batch_processor_queue_size
```

---

## Troubleshooting Performance

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–∏–∑–∫–∏–π throughput

**Symptoms:**
- <5 req/s actual vs expected >10 req/s

**Solutions:**
1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ AsyncOllamaClient
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batch processing
3. –£–≤–µ–ª–∏—á–∏—Ç—å rate limits
4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Ollama capacity

### –ü—Ä–æ–±–ª–µ–º–∞: –í—ã—Å–æ–∫–∞—è latency

**Symptoms:**
- P95 >5s when expected <2s

**Solutions:**
1. –£–≤–µ–ª–∏—á–∏—Ç—å cache size/TTL
2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å prompts (–∫–æ—Ä–æ—á–µ)
3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Ollama –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–∏–∑–∫–∏–π cache hit rate

**Symptoms:**
- <20% hit rate

**Solutions:**
1. –£–≤–µ–ª–∏—á–∏—Ç—å max_size
2. –£–≤–µ–ª–∏—á–∏—Ç—å ttl_seconds
3. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å prompts (—É–±—Ä–∞—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–∏)

---

## Migration Guide

### –ü–µ—Ä–µ—Ö–æ–¥ Sync ‚Üí Async

```python
# Before (sync)
from core.ollama_client import OllamaClient

client = OllamaClient(cfg)
result = client.chat("Hello")

# After (async)
from core.async_ollama_client import AsyncOllamaClient
import asyncio

async def main():
    async with AsyncOllamaClient(cfg) as client:
        result = await client.chat("Hello")

asyncio.run(main())
```

### –ü–µ—Ä–µ—Ö–æ–¥ Async ‚Üí Batch

```python
# Before (async)
results = []
for prompt in prompts:
    result = await client.chat(prompt)
    results.append(result)

# After (batch)
processor = AsyncBatchProcessor(
    process_fn=client.chat_batch,
    batch_size=10,
)

await processor.start()

futures = [processor.submit(p) for p in prompts]
results = await asyncio.gather(*futures)
```

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

- `core/async_ollama_client.py` - Async client
- `core/batch_processor.py` - Batch processor
- `core/ollama_client.py` - Sync client
- `core/llm_cache.py` - Caching
- `core/rate_limiter.py` - Rate limiting
- `docs/metrics-monitoring.md` - Monitoring

## Issues Resolved

- ‚úÖ TD-019 (P0): Async optimization
- ‚úÖ TD-022 (P1): Batch processing
- ‚úÖ TD-020 (P1): Connection pooling

---

**–°–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–æ –º–∞–∫—Å–∏–º—É–º–∞!** üöÄ

**10x+ throughput improvement available** ‚ö°
